#!/usr/bin/env pythonimport sysimport os.pathimport osimport csvimport globfrom pandas import DataFrame, Seriesimport pandas as pdimport matplotlib.pyplot as pltimport numpy as npfrom sklearn import datasets, linear_modelfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysisimport scipy.stats as statsimport numpy.ma as ma# Here's a starter for the homework.  You can call this file from them command# line with a file path as a command-line argument# As you complete the assignment, you can un-comment these lines to pull it all together# frame = read_all_files(sys.argv[1])# num_iv_columns = len(frame.columns)-1# training_class = categorical_transformation(frame)# classify(frame, num_iv_columns, training_class)# Building a Classification Tool# then choose and run a classification algorithm.  It won't be that# statistically rigorous, but that's not our concern here.  Instead, we'll# be building a clean piece of code that separates a number of different# pieces of functionality.# Copy this starter to a python executable named:  classification.py.# Place all functions for in classification.py# The data files for this assignment are data_banknote_authorization.txt#  and iris.csv and they are on Canvas.  They are both publicly available as# documented here:#	https://archive.ics.uci.edu/ml/datasets.html# 0. Reading in the Data.# Copy in the function read_all_files from Lecture 7 and modify it# so that it can either read a single data file OR read all of the files in# a directory.  The function should still take in a single argument path# which can now be either the path to a file or the path to a directory.# The function's signature shoul be exactly:def read_all_files(path):    files = dir_or_file(path)    frames = []    for f in files:        extension = os.path.splitext(f)[-1]        file_reader = reader(extension)        new_frame = file_reader(f)        frames.append(new_frame)    return pd.concat(frames, ignore_index = True)def dir_or_file(path):    if os.path.isdir(path):        if os.listdir(path):            path = path + '/*'        else:            raise Exception ('Folder %s is empty' % path)    elif os.path.isfile(path):        path = path    else:        raise Exception (' %s is not a folder or file. Try another.' % path)    files = glob.glob(path)    return filesdef reader(extension):    if extension == '.csv' or extension == '.txt':        return csv_reader    elif extension == '.json':        return json_reader    else:        raise Exception('Reader for file type %s not found.' % extension)def json_reader(path):    return pd.read_json(path, orient='records')def csv_reader(path):    return pd.read_csv(path)# When reading files from a directory you can assume they are all the same# format and that there is no header.# Your function should return a single data frame containing all of the data.# Your function only needs to be able to read in CSV data.  You can re-use# code from the Lecture and pair programming for this.  Your function should# also be able to read files with the extension .txt as CSV files.  You can# assume there is no header in the CSV data.# Your function should:#  * Raise an exception if the given file type is not supported (i.e. is not CSV).#  * Raise an exception if path is not a file or a directory.#  * Raise an exception if path is an empty directory.# For all three of these exceptions, the value of path should appear in the# error message.# To test your function, you can read both of the test data sets individually.# You can also put a few copies of one of the datasets in a new directory.# --------------------------------------------------------------------------- ## To make things easier we're going to make some assumptions about our data# from here on:#  1. The class we have observed is always on the right hand side of the dataset.#     It is the last column.#  2. All of the other columns are independent variables.  All of those are#     numeric and can be used in our classification.#  e.g. for the row of data:#        3.6216,8.6661,-2.8073,-0.44699,0#  The last 0 is the class we have observed. All the other columns are#  independent variables.# 1. Prepare your data frame.# In order to be able to use any data set on our classification algorithms,# we might need to do a lot of dataset preparation.  We might want to make# sure that our independent variables are numeric or, if they are categorical,# handle them differently in some way.# On this assignment we're not going to go through all of these steps.# Instead, we'll just do one data cleaning step to show the idea.# In our Iris dataset, the classes are described by string values# (such as 'Iris-virginica').  It can often be easier for the classes to have# integer values, so let's create a new column in the data frame that is the# observed class, represented as an integer.#Write a function called (exactly):def categorical_tranformation(frame):    frame['Class Integer']= pd.Categorical.from_array(frame.ix[:,-1]).codes    return frame.ix[:,-1]# that takes a dataframe, frame and returns a new column that has been added# to that dataframe.# Assume that the observed class for each row is in the last column.  Your# function should add a new column to the dataframe and the values of that# column should be an integer representation of the classes.  In our Iris data,# there are three classes so for that dataset your dataframe should have a new# column with values 0, 1 or 2.  In the banknote data, the values should be# binary.# Make sure that the mapping of string class names to integers is not# hard-coded, i.e. your function should work even if there were a new type of# Iris in the data.  Your function should work if the classes are represented# by strings or integers.  Your function should work whether the data is sorted# or not.# Your function should return the new column.# 2. Set up two different Estimators# Write two functions called (exactly):def classify_logistic (frame):    log_estimator = linear_model.LinearRegression()    return log_estimatordef classify_lda(frame):     LDA_estimator = LinearDiscriminantAnalysis()     return LDA_estimator# Both functions should take a DataFrame, frame# The first function returns a sklearn Estmimator object for Logistic Regression.# The second function should return a sklearn Estimator object for LDA.# Note that this step does not require you to fit the data.# 3. Writing a function to choose an algorithm.# Choice of a classification algorithm should depend on many factors and a# practitioner would evaluate various attributes of the data to make that choice.# For this assignment we'll make a simple selection based on one property of LDA.# LDA assumes normality of the independent variables.# Write a function called (exactly):def algorithm_chooser(frame, num_ivs):    for column in frame.ix[:,0:num_ivs]: #TODO fix header problem        threshold = float(0.05)        z, pval = stats.stats.normaltest(frame[column])        pval = np.float(pval)        if (pval > threshold):            return classify_logistic(frame)            break        else:            return classify_lda(frame)# that takes a dataframe, frame and outputs the name of one of your two#classification functions from Section (2).# Your function should select logistic regression if any of the independent# variable columns are not normally distributed.  If all columns are normally# distributed, it should select LDA.  We'll assume that a variable is normally# distributed if its p-value is > 0.05.# Hint:  The SciPy library has a stats package that may be useful in testing normality.# Note:  It's not a requirement of this assignment but it's preferable to stop# checking the normality of columns as soon as you've found one that has# violated that assumption.  You can use a break statement for this.# 4. Putting it all together.# Finally, let's fit the estimator.  Write a function called (exactly):def classify(frame, num_ivs, training_class_series):    my_estimator = algorithm_chooser(frame, num_ivs)    my_estimator.fit(frame.ix[:,0:num_ivs], training_class_series)    return my_estimatornum_ivs = 4frame = read_all_files(sys.argv[1])# categorical_tranformation(frame)# classify_logistic (frame)# classify_lda(frame)# algorithm_chooser(frame, num_ivs)classify(read_all_files(sys.argv[1]),num_ivs, categorical_tranformation(frame))# that takes a dataframe, frame, an integer num_ivs and a pandas Series# training_class_series.  num_ivs is the number of independent variables and# training_class_series is the column of the dataframe that contains the# observed integer class values.# Your function should:#  * Call the algorithm_chooser to choose a function#  * Call the function returned by algorithm_chooser to create an Estimator#  * Call the .fit function on the estimator based on the number of independent#    variables and the observed class values#  * Return the estimator# 5. Questions#  In a comment block, write answers to the following questions:#  (Put answers at the end of your .py file in comments)# a) Is your code maintainable?  Why/give an example?        # Yes, the categorical_tranformation function can categorize any independent variable string        # for any column in the data just by changing the frame indexing.# b) Is your code extensible?  Why/give an example?        #Yes, we could easily create functions for different types of estimators.# c) Is your code reusable?  Why/give an example?        #Yes, the first five functions can take any .csv, .txt, or .json and return a pandas dataframe.